
/*

//to expose port 8070 from hbase connection:
ssh -i dasnes -L 8070:ec2-52-15-169-10.us-east-2.compute.amazonaws.com:8070 hadoop@ec2-52-15-169-10.us-east-2.compute.amazonaws.com
web client should take in a zone and display all rows + timestamps for that row
*/

/*

setup a new kafka topic for lambda function to write to
"topic_dasnes_transcription_finished"

created the topic by going to cluster and
cd /home/hadoop/kafka_2.12-2.2.1/bin/

then
(can abbreviate connection string with just: Kafka-Zookeepers)
./kafka-topics.sh --create --zookeeper z-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:2181,z-3.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:2181,z-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:2181 --replication-factor 1 --partitions 1 --topic topic_dasnes_transcription_finished


NEW TOPICS FOR WEB APP
topic_dasnes_web_upload_with_audio
topic_dasnes_web_upload_no_audio


and verify it is there by running:

./kafka-topics.sh --list --zookeeper z-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:2181,z-3.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:2181,z-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:2181 | grep dasnes


I can produce to the topic with 
./kafka-console-producer.sh --broker-list b-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092,b-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092 --topic topic_dasnes_transcription_finished

And I can consume from that topic with:
./kafka-console-consumer.sh --bootstrap-server b-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092,b-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092 --topic topic_dasnes_transcription_finished --from-beginning


Run the spark submit job as such:
spark-submit --master local[2] --driver-java-options "-Dlog4j.configuration=file:///home/hadoop/ss.log4j.properties" --class StreamWeather uber-kafka-consumer-2-1.0-SNAPSHOT.jar b-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092,b-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092


spark-submit --master local[2] --driver-java-options "-Dlog4j.configuration=file:///home/hadoop/ss.log4j.properties" --class ProcessLiveWebInput uber-web-text-input-kafka-consumer-1.0-SNAPSHOT.jar b-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092,b-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092
^ which requires listening on 
./kafka-console-consumer.sh --bootstrap-server b-2.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092,b-1.mpcs53014-kafka.fwx2ly.c4.kafka.us-east-2.amazonaws.com:9092 --topic topic_dasnes_web_upload_no_audio --from-beginning

test by:
enter this in the producer: {"zone_timestamp": "12345", "text": "my text"}
and can check in hbase by: get 'dasnes_proj_csv_as_hbase', '12345'

can test node on load balanced servers here: 
http://mpcs53014-loadbalancer-217964685.us-east-2.elb.amazonaws.com:3005

*/


/*
update the python code in a lambda by running:
aws lambda update-function-code --function-name dasnes-write-uri-to-kafka-topic-on-transcribe-finish --zip-file fileb://my-deployment-package.zip

*/

/*
starting with just one hive table:

id | dept name | zone | time of day | date of event | duration | text

I simulated source data to match this schema, then moved it into AWS
From there, I ssh'd into the cluster and ran this command to put that csv into hdfs
#first create the new dir
hdfs dfs -mkdir /tmp/dasnes-final-project/sample-data/starter-data-final-schema/
#then copy the data over
aws s3 cp s3://dasnes-mpcs53014/starter_data_final_schema.csv /dev/stdout | hdfs dfs -put - /tmp/dasnes-final-project/sample-data/starter-data/
*/




